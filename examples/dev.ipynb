{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"XLA_PYTHON_CLIENT_PREALLOCATE\"] = \"false\"\n",
    "os.environ[\"XLA_PYTHON_CLIENT_ALLOCATOR\"]= \"platform\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "from einops import rearrange\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# jax.config.update('jax_platform_name', 'cpu')\n",
    "# jax.config.update(\"jax_enable_x64\", True)  # Enables float64 precision globally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "@partial(jax.jit, static_argnames=[\"pool\", \"win\", \"stride\", \"pad\"])\n",
    "def shrink(X, pool, win, stride, pad):\n",
    "    \"\"\"\n",
    "    Apply shrink operation using max pooling and neighborhood construction.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : jnp.ndarray\n",
    "        Input array of shape (batch, height, width, channels).\n",
    "    pool : int\n",
    "        Pooling window size.\n",
    "    win : int\n",
    "        Neighborhood window size.\n",
    "    stride : int\n",
    "        Stride size for neighborhood construction.\n",
    "    pad : int\n",
    "        Padding size.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    jnp.ndarray\n",
    "        Transformed array.\n",
    "    \"\"\"\n",
    "\n",
    "    # ---- max pooling ----\n",
    "    X = jax.lax.reduce_window(\n",
    "        X,\n",
    "        -jnp.inf,\n",
    "        jax.lax.max,\n",
    "        (1, pool, pool, 1),\n",
    "        (1, pool, pool, 1),\n",
    "        padding=\"VALID\",\n",
    "    )\n",
    "\n",
    "    # ---- neighborhood construction ----\n",
    "    X = rearrange(X, \"b h w c -> b c h w\")\n",
    "    X = jnp.pad(X, ((0, 0), (0, 0), (pad, pad), (pad, pad)), mode=\"reflect\")\n",
    "    X = jax.lax.conv_general_dilated_patches(\n",
    "        lhs=X, filter_shape=(win, win), window_strides=(stride, stride), padding=\"VALID\"\n",
    "    )\n",
    "    X = rearrange(X, \"b (c p) h w -> b h w p c\", p=win**2)\n",
    "    return X\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "@partial(jax.jit, static_argnames=[\"win\", \"stride\"])\n",
    "def extract_patches(X, win, stride):\n",
    "    B, H, W, C = X.shape\n",
    "    out_h = (H - win) // stride + 1\n",
    "    out_w = (W - win) // stride + 1\n",
    "\n",
    "    @partial(jax.jit)\n",
    "    def get_patch(i, j, X):\n",
    "        \"\"\"Dynamically extract a patch at position (i, j).\"\"\"\n",
    "        return jax.lax.dynamic_slice(X, (0, i * stride, j * stride, 0), (B, win, win, C))\n",
    "\n",
    "    # Vectorized patch extraction\n",
    "    i_vals = jnp.arange(out_h)\n",
    "    j_vals = jnp.arange(out_w)\n",
    "    patches = jax.vmap(lambda i: jax.vmap(lambda j: get_patch(i, j, X))(j_vals))(i_vals)\n",
    "\n",
    "    return patches.reshape(B * out_h * out_w, win * win * C)\n",
    "\n",
    "@partial(jax.jit, static_argnames=[\"pool\", \"win\", \"stride\", \"pad\", \"batch_size\"])\n",
    "def compute_patch_stats(X, pool, win, stride, pad, batch_size):\n",
    "    B, H, W, C = X.shape\n",
    "    # X = X.astype(jnp.float32)\n",
    "    num_kernel = win * win * C\n",
    "    num_batches = max(B // batch_size, 1)\n",
    "    X = X[:num_batches * batch_size]\n",
    "\n",
    "    # ---- Apply Max Pooling ----\n",
    "    X = jax.lax.reduce_window(\n",
    "        X, -jnp.inf, jax.lax.max,\n",
    "        (1, pool, pool, 1),  # Pooling size\n",
    "        (1, pool, pool, 1),  # Stride\n",
    "        padding=\"VALID\"\n",
    "    )\n",
    "\n",
    "    # ---- Apply Padding ----\n",
    "    X = jnp.pad(X, ((0, 0), (pad, pad), (pad, pad), (0, 0)), mode=\"reflect\")\n",
    "\n",
    "    X_batch = jnp.split(X, num_batches)\n",
    "\n",
    "    # ---- Compute Statistics ----\n",
    "    dc_batch = []\n",
    "    bias = 0\n",
    "    mean = jnp.zeros((1, num_kernel))\n",
    "    for X in X_batch:\n",
    "        patches = extract_patches(X, win, stride)\n",
    "        dc_local = jnp.mean(patches, axis=-1, keepdims=-1)\n",
    "        X_centered = patches - dc_local\n",
    "        bias_local = jnp.max(jnp.linalg.norm(X_centered, axis=-1))\n",
    "        mean_local = jnp.mean(X_centered, axis=0, keepdims=True)\n",
    "\n",
    "        dc_batch.append(dc_local)\n",
    "        bias = jnp.maximum(bias_local, bias)\n",
    "        mean += mean_local / num_batches\n",
    "\n",
    "    # ---- Compute Covariance ----\n",
    "    covariance = jnp.zeros((num_kernel, num_kernel))\n",
    "    for X, dc in zip(X_batch, dc_batch):\n",
    "        patches = extract_patches(X, win, stride)\n",
    "        X_centered = (patches - dc - mean)\n",
    "        covariance += X_centered.T @ X_centered\n",
    "        # covariance += jnp.einsum(\"ni,nj->ij\", patches - dc - mean, patches - dc - mean)\n",
    "\n",
    "\n",
    "    covariance = covariance / (batch_size  * num_batches * H * W - 1)\n",
    "    return covariance, mean\n",
    "    # return mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "@partial(jax.jit, static_argnames=[\"win\", \"stride\"])\n",
    "def _extract_patches(X, win, stride):\n",
    "    \"\"\"\n",
    "    Extracts sliding patches dynamically from input X using `jax.lax.dynamic_slice`.\n",
    "\n",
    "    Args:\n",
    "        X: Input tensor (B, H, W, C)\n",
    "        win: Patch size.\n",
    "        stride: Stride for sliding window.\n",
    "\n",
    "    Returns:\n",
    "        Patches of shape (B * out_H * out_W, win * win * C)\n",
    "    \"\"\"\n",
    "    B, H, W, C = X.shape\n",
    "    out_h = (H - win) // stride + 1\n",
    "    out_w = (W - win) // stride + 1\n",
    "\n",
    "    def get_patch(i, j, X):\n",
    "        return jax.lax.dynamic_slice(X, (0, i * stride, j * stride, 0), (B, win, win, C))\n",
    "\n",
    "    # Vectorized patch extraction\n",
    "    i_vals, j_vals = jnp.arange(out_h), jnp.arange(out_w)\n",
    "    patches = jax.vmap(lambda i: jax.vmap(lambda j: get_patch(i, j, X))(j_vals))(i_vals)\n",
    "\n",
    "    return patches.reshape(B * out_h * out_w, win * win * C)\n",
    "\n",
    "@partial(jax.jit, static_argnames=[\"extract_patches\", \"num_kernel\"])\n",
    "def _compute_statistics(X_batch, extract_patches, num_kernel):\n",
    "    \"\"\"\n",
    "    Computes local mean (dc_local), bias, and mean update for each batch.\n",
    "\n",
    "    Args:\n",
    "        X_batch: Batched input tensor.\n",
    "        win: Patch size.\n",
    "        stride: Stride for patches.\n",
    "        num_batches: Number of mini-batches.\n",
    "        num_kernel: Number of elements in a single patch.\n",
    "\n",
    "    Returns:\n",
    "        mean, bias, dc_batch\n",
    "    \"\"\"\n",
    "    def scan_statistics(carry, X_cur):\n",
    "        mean, bias = carry  # Unpack carry state\n",
    "        patches = extract_patches(X_cur)\n",
    "        dc_local = jnp.mean(patches, axis=-1, keepdims=True)  # Mean per patch\n",
    "        X_centered = patches - dc_local\n",
    "\n",
    "        bias_local = jnp.max(jnp.linalg.norm(X_centered, axis=-1))\n",
    "        mean_local = jnp.mean(X_centered, axis=0, keepdims=True)\n",
    "\n",
    "        new_mean = mean + mean_local\n",
    "        new_bias = jnp.maximum(bias, bias_local)\n",
    "\n",
    "        return (new_mean, new_bias), dc_local\n",
    "\n",
    "    mean_init = jnp.zeros((1, num_kernel))\n",
    "    bias_init = 0.0\n",
    "\n",
    "    (final_mean, final_bias), dc_batch = jax.lax.scan(scan_statistics, (mean_init, bias_init), X_batch)\n",
    "    final_mean /= len(X_batch)\n",
    "    \n",
    "    return final_mean, final_bias, dc_batch\n",
    "\n",
    "@partial(jax.jit, static_argnames=[\"num_kernel\", \"extract_patches\"])\n",
    "def _compute_covariance(X_batch, dc_batch, mean, extract_patches,  num_kernel):\n",
    "    def scan_covariance(carry, inputs):\n",
    "        X_cur, dc = inputs\n",
    "        patches = extract_patches(X_cur)\n",
    "        X_centered = patches - dc - mean\n",
    "\n",
    "        carry += jnp.einsum(\"...i,...j->ij\", X_centered, X_centered, precision=jax.lax.Precision.HIGHEST)\n",
    "        return carry, None\n",
    "\n",
    "    covariance_init = jnp.zeros((num_kernel, num_kernel))\n",
    "\n",
    "    covariance, _ = jax.lax.scan(scan_covariance, covariance_init, (X_batch, dc_batch))\n",
    "\n",
    "    return covariance \n",
    "\n",
    "@partial(jax.jit, static_argnames=[\"pool\", \"win\", \"stride\", \"pad\", \"batch_size\"])\n",
    "def compute_patch_stats(X, pool, win, stride, pad, batch_size):\n",
    "    \"\"\"\n",
    "    Compute the mean, bias, and covariance of sliding patches.\n",
    "\n",
    "    Args:\n",
    "        X: Input tensor (B, H, W, C)\n",
    "        pool: Pooling size.\n",
    "        win: Patch size.\n",
    "        stride: Stride for patches.\n",
    "        pad: Padding size.\n",
    "        batch_size: Batch size.\n",
    "\n",
    "    Returns:\n",
    "        covariance matrix, mean\n",
    "    \"\"\"\n",
    "    B, H, W, C = X.shape\n",
    "    num_kernel = win * win * C\n",
    "    num_batches = max(B // batch_size, 1)\n",
    "    X = X[:num_batches * batch_size]\n",
    "\n",
    "    # ---- Apply Max Pooling ----\n",
    "    X = jax.lax.reduce_window(\n",
    "        X, -jnp.inf, jax.lax.max,\n",
    "        (1, pool, pool, 1),\n",
    "        (1, pool, pool, 1),\n",
    "        padding=\"VALID\"\n",
    "    )\n",
    "\n",
    "    # ---- Apply Padding ----\n",
    "    X = jnp.pad(X, ((0, 0), (pad, pad), (pad, pad), (0, 0)), mode=\"reflect\")\n",
    "\n",
    "    X_batch = jnp.stack(jnp.split(X, num_batches))\n",
    "\n",
    "    # ---- Compute Mean and Bias ----\n",
    "    mean, bias, dc_batch = _compute_statistics(X_batch, lambda X: _extract_patches(X, win, stride), num_kernel)\n",
    "\n",
    "    # ---- Compute Covariance ----\n",
    "    covariance = _compute_covariance(X_batch, dc_batch, mean, lambda X: _extract_patches(X, win, stride), num_kernel)\n",
    "    covariance /= (batch_size * num_batches * H * W - 1)\n",
    "\n",
    "    return covariance, mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "@partial(jax.jit, static_argnames=[\"pool\", \"win\", \"stride\", \"pad\"])\n",
    "def compute(X, pool, win, stride, pad):\n",
    "    # X = X.astype(jnp.float32)\n",
    "    X = shrink(X, pool, win, stride, pad)\n",
    "    X = rearrange(X, \"... p c -> (...) (p c)\")\n",
    "\n",
    "    dc = jnp.mean(X, axis=-1, keepdims=-1)\n",
    "    X = X - dc\n",
    "    bias = jnp.max(jnp.linalg.norm(X, axis=-1))\n",
    "    mean = jnp.mean(X, axis=0, keepdims=True)\n",
    "\n",
    "    # covariance = (X - mean).T @ (X - mean)\n",
    "    covariance = jnp.einsum(\"ni,nj->ij\", X - mean, X - mean)\n",
    "    covariance /= (X.shape[0] - 1)\n",
    "    return covariance, mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.random.randn(50000, 128, 128, 3) + 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'compute' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m a = \u001b[43mcompute\u001b[49m(X, \u001b[32m1\u001b[39m, \u001b[32m7\u001b[39m, \u001b[32m1\u001b[39m, \u001b[32m3\u001b[39m)\n\u001b[32m      2\u001b[39m a[\u001b[32m0\u001b[39m].block_until_ready()\n",
      "\u001b[31mNameError\u001b[39m: name 'compute' is not defined"
     ]
    }
   ],
   "source": [
    "a = compute(X, 1, 7, 1, 3)\n",
    "a[0].block_until_ready()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "ename": "EinopsError",
     "evalue": " Error while processing rearrange-reduction pattern \"n (p p) c -> n p p c\".\n Input tensor shape: (10, 100, 10). Additional info: {}.\n Indexing expression contains duplicate dimension \"p\"",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mEinopsError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/generation/lib/python3.12/site-packages/einops/einops.py:531\u001b[39m, in \u001b[36mreduce\u001b[39m\u001b[34m(tensor, pattern, reduction, **axes_lengths)\u001b[39m\n\u001b[32m    530\u001b[39m shape = backend.shape(tensor)\n\u001b[32m--> \u001b[39m\u001b[32m531\u001b[39m recipe = \u001b[43m_prepare_transformation_recipe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpattern\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxes_names\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43maxes_lengths\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mndim\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mshape\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    532\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m _apply_recipe(\n\u001b[32m    533\u001b[39m     backend, recipe, cast(Tensor, tensor), reduction_type=reduction, axes_lengths=hashable_axes_lengths\n\u001b[32m    534\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/generation/lib/python3.12/site-packages/einops/einops.py:300\u001b[39m, in \u001b[36m_prepare_transformation_recipe\u001b[39m\u001b[34m(pattern, operation, axes_names, ndim)\u001b[39m\n\u001b[32m    299\u001b[39m left_str, rght_str = pattern.split(\u001b[33m\"\u001b[39m\u001b[33m->\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m300\u001b[39m left = \u001b[43mParsedExpression\u001b[49m\u001b[43m(\u001b[49m\u001b[43mleft_str\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    301\u001b[39m rght = ParsedExpression(rght_str)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/generation/lib/python3.12/site-packages/einops/parsing.py:88\u001b[39m, in \u001b[36mParsedExpression.__init__\u001b[39m\u001b[34m(self, expression, allow_underscore, allow_duplicates)\u001b[39m\n\u001b[32m     87\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m current_identifier \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m88\u001b[39m     \u001b[43madd_axis_name\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcurrent_identifier\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     89\u001b[39m current_identifier = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/generation/lib/python3.12/site-packages/einops/parsing.py:53\u001b[39m, in \u001b[36mParsedExpression.__init__.<locals>.add_axis_name\u001b[39m\u001b[34m(x)\u001b[39m\n\u001b[32m     52\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (allow_underscore \u001b[38;5;129;01mand\u001b[39;00m x == \u001b[33m\"\u001b[39m\u001b[33m_\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m allow_duplicates:\n\u001b[32m---> \u001b[39m\u001b[32m53\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m EinopsError(\u001b[33m'\u001b[39m\u001b[33mIndexing expression contains duplicate dimension \u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m\"\u001b[39m\u001b[33m'\u001b[39m.format(x))\n\u001b[32m     54\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m x == _ellipsis:\n",
      "\u001b[31mEinopsError\u001b[39m: Indexing expression contains duplicate dimension \"p\"",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mEinopsError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[181]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m a = np.ones((\u001b[32m10\u001b[39m, \u001b[32m100\u001b[39m, \u001b[32m10\u001b[39m))\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[43mrearrange\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mn (p p) c -> n p p c\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m.shape\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/generation/lib/python3.12/site-packages/einops/einops.py:600\u001b[39m, in \u001b[36mrearrange\u001b[39m\u001b[34m(tensor, pattern, **axes_lengths)\u001b[39m\n\u001b[32m    545\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mrearrange\u001b[39m(tensor: Union[Tensor, List[Tensor]], pattern: \u001b[38;5;28mstr\u001b[39m, **axes_lengths: Size) -> Tensor:\n\u001b[32m    546\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    547\u001b[39m \u001b[33;03m    einops.rearrange is a reader-friendly smart element reordering for multidimensional tensors.\u001b[39;00m\n\u001b[32m    548\u001b[39m \u001b[33;03m    This operation includes functionality of transpose (axes permutation), reshape (view), squeeze, unsqueeze,\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    598\u001b[39m \n\u001b[32m    599\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m600\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mreduce\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpattern\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrearrange\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43maxes_lengths\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/generation/lib/python3.12/site-packages/einops/einops.py:542\u001b[39m, in \u001b[36mreduce\u001b[39m\u001b[34m(tensor, pattern, reduction, **axes_lengths)\u001b[39m\n\u001b[32m    540\u001b[39m     message += \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m Input is list. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    541\u001b[39m message += \u001b[33m\"\u001b[39m\u001b[33mAdditional info: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m.format(axes_lengths)\n\u001b[32m--> \u001b[39m\u001b[32m542\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m EinopsError(message + \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m\"\u001b[39m.format(e))\n",
      "\u001b[31mEinopsError\u001b[39m:  Error while processing rearrange-reduction pattern \"n (p p) c -> n p p c\".\n Input tensor shape: (10, 100, 10). Additional info: {}.\n Indexing expression contains duplicate dimension \"p\""
     ]
    }
   ],
   "source": [
    "a = np.ones((10, 100, 10))\n",
    "rearrange(a, \"n (p p) c -> n p p c\").shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([[ 0.99270904, -0.00725159, -0.00722823, ..., -0.00708457,\n",
       "        -0.00731058, -0.00726368],\n",
       "       [-0.00725159,  0.99279356, -0.00727239, ..., -0.00731609,\n",
       "        -0.00706959, -0.00724925],\n",
       "       [-0.00722823, -0.00727239,  0.9926517 , ..., -0.00736245,\n",
       "        -0.00731914, -0.00707388],\n",
       "       ...,\n",
       "       [-0.00708457, -0.00731609, -0.00736245, ...,  0.992648  ,\n",
       "        -0.00726744, -0.00719396],\n",
       "       [-0.00731058, -0.00706959, -0.00731914, ..., -0.00726744,\n",
       "         0.9928077 , -0.0072573 ],\n",
       "       [-0.00726368, -0.00724925, -0.00707388, ..., -0.00719396,\n",
       "        -0.0072573 ,  0.9926555 ]], dtype=float32)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# jax.profiler.start_trace(\"./jax-trace\")\n",
    "# b = compute_patch_stats(X[:10], 1, 7, 1, 3, 1)\n",
    "# b[0].block_until_ready()\n",
    "\n",
    "b = compute_patch_stats(X, 1, 7, 1, 3, 100)\n",
    "b[0].block_until_ready()\n",
    "# jax.profiler.stop_trace()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array(0.00029544, dtype=float32)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jnp.abs(jnp.load(\"gt.npy\") - b[0]).max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"gt.npy\", b[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "float32\n"
     ]
    }
   ],
   "source": [
    "print(b[0].dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array(False, dtype=bool)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jnp.allclose(np.load(\"b.npy\"), b[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array(151.95303, dtype=float32)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jnp.abs(b[0]-jnp.eye(a[0].shape[0])).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array(True, dtype=bool)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jnp.allclose(a[1], b[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.random.randn(10000, 128, 128, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 10000, 128, 128, 1)\n",
      "(3, 49, 49)\n"
     ]
    }
   ],
   "source": [
    "# jax.profiler.start_trace(\"./jax-trace\")\n",
    "# compute_channel_wise = jax.vmap(lambda X: compute_patch_stats(X, 1, 5, 1, 3, 10))\n",
    "Xt = rearrange(X, \"n h w c ->  c n h w 1\")\n",
    "print(Xt.shape)\n",
    "b = jnp.array([compute_patch_stats(X_channel, 1, 7, 1, 3, 40)[0] for X_channel in Xt])\n",
    "# b = compute_channel_wise(Xt)\n",
    "# b = jax.lax.map(lambda X: compute_patch_stats(X, 1, 5, 1, 3, 10), Xt)\n",
    "b[0].block_until_ready()\n",
    "jax.profiler.save_device_memory_profile(\"memory.prof\", backend=\"gpu\")\n",
    "print(b.shape)\n",
    "# jax.profiler.stop_trace()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.random.randn(5000, 128, 128, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(147, 147)\n"
     ]
    }
   ],
   "source": [
    "# jax.profiler.start_trace(\"./jax-trace\")\n",
    "b = compute_patch_stats(X, 1, 7, 1, 3, 40)\n",
    "b[0].block_until_ready()\n",
    "jax.profiler.save_device_memory_profile(\"memory.prof\", backend=\"gpu\")\n",
    "print(b[0].shape)\n",
    "# jax.profiler.stop_trace()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "generation",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
